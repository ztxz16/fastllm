# fastllm

## ä»‹ç»

fastllmæ˜¯çº¯c++å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ–çš„é«˜æ€§èƒ½å¤§æ¨¡å‹æ¨ç†åº“

6~7Bçº§æ¨¡å‹åœ¨å®‰å“ç«¯ä¸Šä¹Ÿå¯ä»¥æµç•…è¿è¡Œ

éƒ¨ç½²äº¤æµQQç¾¤ï¼š 831641348

| [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) | [æ¨¡å‹è·å–](#æ¨¡å‹è·å–) | [å¼€å‘è®¡åˆ’](#å¼€å‘è®¡åˆ’) |

## åŠŸèƒ½æ¦‚è¿°

- ğŸš€ çº¯c++å®ç°ï¼Œä¾¿äºè·¨å¹³å°ç§»æ¤ï¼Œå¯ä»¥åœ¨å®‰å“ä¸Šç›´æ¥ç¼–è¯‘
- ğŸš€ ARMå¹³å°æ”¯æŒNEONæŒ‡ä»¤é›†åŠ é€Ÿï¼ŒX86å¹³å°æ”¯æŒAVXæŒ‡ä»¤é›†åŠ é€Ÿï¼ŒNVIDIAå¹³å°æ”¯æŒCUDAåŠ é€Ÿï¼Œå„ä¸ªå¹³å°é€Ÿåº¦éƒ½å¾ˆå¿«å°±æ˜¯äº†
- ğŸš€ æ”¯æŒæµ®ç‚¹æ¨¡å‹ï¼ˆFP32), åŠç²¾åº¦æ¨¡å‹(FP16), é‡åŒ–æ¨¡å‹(INT8, INT4) åŠ é€Ÿ
- ğŸš€ æ”¯æŒå¤šå¡éƒ¨ç½²ï¼Œæ”¯æŒGPU + CPUæ··åˆéƒ¨ç½²
- ğŸš€ æ”¯æŒBatché€Ÿåº¦ä¼˜åŒ–
- ğŸš€ æ”¯æŒå¹¶å‘è®¡ç®—æ—¶åŠ¨æ€æ‹¼Batch
- ğŸš€ æ”¯æŒæµå¼è¾“å‡ºï¼Œå¾ˆæ–¹ä¾¿å®ç°æ‰“å­—æœºæ•ˆæœ
- ğŸš€ æ”¯æŒpythonè°ƒç”¨
- ğŸš€ å‰åç«¯åˆ†ç¦»è®¾è®¡ï¼Œä¾¿äºæ”¯æŒæ–°çš„è®¡ç®—è®¾å¤‡
- ğŸš€ ç›®å‰æ”¯æŒChatGLMç³»åˆ—æ¨¡å‹ï¼Œå„ç§LLAMAæ¨¡å‹(ALPACA, VICUNAç­‰)ï¼ŒBAICHUANæ¨¡å‹ï¼ŒQWENæ¨¡å‹ï¼ŒMOSSæ¨¡å‹ï¼ŒMINICPMæ¨¡å‹ç­‰

## ä¸¤è¡Œä»£ç åŠ é€Ÿ ï¼ˆæµ‹è¯•ä¸­ï¼Œæš‚æ—¶åªæ”¯æŒchatglmç³»åˆ—ï¼‰

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…fastllm_pytoolsåŒ…

``` sh
cd fastllm
mkdir build
cd build
cmake .. -DUSE_CUDA=ON # å¦‚æœä¸ä½¿ç”¨GPUç¼–è¯‘ï¼Œé‚£ä¹ˆä½¿ç”¨ cmake .. -DUSE_CUDA=OFF
make -j
cd tools && python setup.py install
```

ç„¶ååªéœ€è¦åœ¨åŸæœ¬çš„æ¨ç†ç¨‹åºä¸­åŠ å…¥ä¸¤è¡Œå³å¯ä½¿ç”¨fastllmåŠ é€Ÿ

``` python
# è¿™æ˜¯åŸæ¥çš„ç¨‹åºï¼Œé€šè¿‡huggingfaceæ¥å£åˆ›å»ºæ¨¡å‹
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code = True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code = True)

# åŠ å…¥ä¸‹é¢è¿™ä¸¤è¡Œï¼Œå°†huggingfaceæ¨¡å‹è½¬æ¢æˆfastllmæ¨¡å‹
# ç›®å‰from_hfæ¥å£åªèƒ½æ¥å—åŸå§‹æ¨¡å‹ï¼Œæˆ–è€…ChatGLMçš„int4, int8é‡åŒ–æ¨¡å‹ï¼Œæš‚æ—¶ä¸èƒ½è½¬æ¢å…¶å®ƒé‡åŒ–æ¨¡å‹
from fastllm_pytools import llm
model = llm.from_hf(model, tokenizer, dtype = "float16") # dtypeæ”¯æŒ "float16", "int8", "int4"

# æ³¨é‡Šæ‰è¿™ä¸€è¡Œmodel.eval()
#model = model.eval()
```

modelæ”¯æŒäº†ChatGLMçš„APIå‡½æ•°chat, stream_chatï¼Œå› æ­¤ChatGLMçš„demoç¨‹åºæ— éœ€æ”¹åŠ¨å…¶ä»–ä»£ç å³å¯è¿è¡Œ

modelè¿˜æ”¯æŒä¸‹åˆ—APIç”¨äºç”Ÿæˆå›å¤

``` python
# ç”Ÿæˆå›å¤
print(model.response("ä½ å¥½"))

# æµå¼ç”Ÿæˆå›å¤
for response in model.stream_response("ä½ å¥½"):
    print(response, flush = True, end = "")
```

è½¬å¥½çš„æ¨¡å‹ä¹Ÿå¯ä»¥å¯¼å‡ºåˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä¹‹åå¯ä»¥ç›´æ¥è¯»å–ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨fastllm cppæ¥å£è¯»å–

``` python
model.save("model.flm"); # å¯¼å‡ºfastllmæ¨¡å‹
new_model = llm.model("model.flm"); # å¯¼å…¥fastllmæ¨¡å‹
```

æ³¨: è¯¥åŠŸèƒ½å¤„äºæµ‹è¯•é˜¶æ®µï¼Œç›®å‰ä»…éªŒè¯äº†ChatGLMã€ChatGLM2æ¨¡å‹å¯ä»¥é€šè¿‡2è¡Œä»£ç åŠ é€Ÿ

## PEFTæ”¯æŒ(æµ‹è¯•ä¸­ï¼Œç›®å‰ä»…æ”¯æŒChatGLM + LoRA)

ä½¿ç”¨[ğŸ¤—PEFT](https://huggingface.co/docs/peft/index)å¯ä»¥æ–¹ä¾¿åœ°è¿è¡Œfinetuneè¿‡çš„å¤§æ¨¡å‹ï¼Œä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹çš„æ–¹å¼è®©ä½ çš„PEFTæ¨¡å‹ä½¿ç”¨fastllmåŠ é€Ÿï¼š

```python
import sys
from peft import PeftModel
from transformers import AutoModel, AutoTokenizer
sys.path.append('..')
model = AutoModel.from_pretrained("THUDM/chatglm-6b", device_map='cpu', trust_remote_code=True)
model = PeftModel.from_pretrained(model, "path/to/your/own/adapter") # è¿™é‡Œä½¿ç”¨ä½ è‡ªå·±çš„peft adapter
model = model.eval()
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)

# å¦‚æœæ¨¡å‹ä¸­å­˜åœ¨active_adapterï¼Œé‚£ä¹ˆåœ¨fastllmæ¨¡å‹ä¸­ï¼Œè¿™ä¸ªadapterä¹Ÿä¼šè¢«é»˜è®¤å¯ç”¨
from fastllm_pytools import llm
model = llm.from_hf(model, tokenizer, dtype = "float16") # dtypeæ”¯æŒ "float16", "int8", "int4"
```

æ¥ä¸‹æ¥ï¼Œä½ å°±å¯ä»¥åƒä½¿ç”¨æ™®é€šçš„æ¨¡å‹ä¸€æ ·(ä¾‹å¦‚è°ƒç”¨chatï¼Œstream_chatå‡½æ•°)

ä½ ä¹Ÿå¯ä»¥æ›´æ¢PEFTæ¨¡å‹æ‰€ä½¿ç”¨çš„çš„adapterï¼š

```python
model.set_adapter('your adapter name')
```

æˆ–è€…å…³é—­PEFTï¼Œä½¿ç”¨åŸæœ¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼š

```python
model.disable_adapter()
```

## æ¨ç†é€Ÿåº¦

6Bçº§int4æ¨¡å‹å•4090å»¶è¿Ÿæœ€ä½çº¦5.5ms

6Bçº§fp16æ¨¡å‹å•4090æœ€å¤§ååé‡è¶…è¿‡10000 token / s

6Bçº§int4æ¨¡å‹åœ¨éªé¾™865ä¸Šé€Ÿåº¦å¤§çº¦ä¸º4~5 token / s

[è¯¦ç»†æµ‹è¯•æ•°æ®ç‚¹è¿™é‡Œ](docs/benchmark.md)

## CMMLUç²¾åº¦æµ‹è¯•

|              æ¨¡å‹  | Dataç²¾åº¦ |  CMMLUåˆ†æ•° |
|-----------------: |-------- |------------|
| ChatGLM2-6b-fp16  | float32 |  50.16     |
| ChatGLM2-6b-int8  | float32 |  50.14     |
| ChatGLM2-6b-int4  | float32 |  49.63     |

ç›®å‰æµ‹è¯•äº†ChatGLM2æ¨¡å‹ï¼Œå…·ä½“æµ‹è¯•æ­¥éª¤ç‚¹[è¿™é‡Œ](test/cmmlu/README.md)

## å¿«é€Ÿå¼€å§‹

### ç¼–è¯‘

å»ºè®®ä½¿ç”¨cmakeç¼–è¯‘ï¼Œéœ€è¦æå‰å®‰è£…c++ç¼–è¯‘å™¨ï¼Œmake, cmake

gccç‰ˆæœ¬å»ºè®®9.4ä»¥ä¸Šï¼Œcmakeç‰ˆæœ¬å»ºè®®3.23ä»¥ä¸Š

GPUç¼–è¯‘éœ€è¦æå‰å®‰è£…å¥½CUDAç¼–è¯‘ç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨å°½å¯èƒ½æ–°çš„CUDAç‰ˆæœ¬

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ç¼–è¯‘

``` sh
cd fastllm
mkdir build
cd build
cmake .. -DUSE_CUDA=ON # å¦‚æœä¸ä½¿ç”¨GPUç¼–è¯‘ï¼Œé‚£ä¹ˆä½¿ç”¨ cmake .. -DUSE_CUDA=OFF
make -j
```

ç¼–è¯‘å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ç®€æ˜“pythonå·¥å…·åŒ…ã€‚

``` sh
cd tools # è¿™æ—¶åœ¨fastllm/build/toolsç›®å½•ä¸‹
python setup.py install
```

å…¶ä»–ä¸åŒå¹³å°çš„ç¼–è¯‘å¯å‚è€ƒæ–‡æ¡£
[TFACCå¹³å°](docs/tfacc.md)

### è¿è¡Œdemoç¨‹åº

æˆ‘ä»¬å‡è®¾å·²ç»è·å–äº†åä¸º`model.flm`çš„æ¨¡å‹ï¼ˆå‚ç…§ [æ¨¡å‹è·å–](#æ¨¡å‹è·å–)ï¼Œåˆæ¬¡ä½¿ç”¨å¯ä»¥å…ˆä¸‹è½½è½¬æ¢å¥½çš„æ¨¡å‹)

ç¼–è¯‘å®Œæˆä¹‹ååœ¨buildç›®å½•ä¸‹å¯ä»¥ä½¿ç”¨ä¸‹åˆ—demo:

``` sh
# è¿™æ—¶åœ¨fastllm/buildç›®å½•ä¸‹

# å‘½ä»¤è¡ŒèŠå¤©ç¨‹åº, æ”¯æŒæ‰“å­—æœºæ•ˆæœ (åªæ”¯æŒLinuxï¼‰
./main -p model.flm 

# ç®€æ˜“webui, ä½¿ç”¨æµå¼è¾“å‡º + åŠ¨æ€batchï¼Œå¯å¤šè·¯å¹¶å‘è®¿é—®
./webui -p model.flm --port 1234 

# pythonç‰ˆæœ¬çš„å‘½ä»¤è¡ŒèŠå¤©ç¨‹åºï¼Œä½¿ç”¨äº†æ¨¡å‹åˆ›å»ºä»¥åŠæµå¼å¯¹è¯æ•ˆæœ
python tools/cli_demo.py -p model.flm 

# pythonç‰ˆæœ¬çš„ç®€æ˜“webuiï¼Œéœ€è¦å…ˆå®‰è£…streamlit-chat
streamlit run tools/web_demo.py model.flm 

```

Windowsä¸‹çš„ç¼–è¯‘æ¨èä½¿ç”¨Cmake GUI + Visual Studioï¼Œåœ¨å›¾å½¢åŒ–ç•Œé¢ä¸­å®Œæˆã€‚

å¦‚ç¼–è¯‘ä¸­å­˜åœ¨é—®é¢˜ï¼Œå°¤å…¶æ˜¯Windowsä¸‹çš„ç¼–è¯‘ï¼Œå¯å‚è€ƒ[FAQ](docs/faq.md)

### ç®€æ˜“pythonè°ƒç”¨

ç¼–è¯‘åå¦‚æœå®‰è£…äº†ç®€æ˜“pythonå·¥å…·åŒ…ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨pythonæ¥è°ƒç”¨ä¸€äº›åŸºæœ¬çš„API ï¼ˆå¦‚æœæ²¡æœ‰å®‰è£…ï¼Œä¹Ÿå¯ä»¥åœ¨ç›´æ¥importç¼–è¯‘ç”Ÿæˆçš„tools/fastllm_pytoolsæ¥ä½¿ç”¨)

``` python
# æ¨¡å‹åˆ›å»º
from fastllm_pytools import llm
model = llm.model("model.flm")

# ç”Ÿæˆå›å¤
print(model.response("ä½ å¥½"))

# æµå¼ç”Ÿæˆå›å¤
for response in model.stream_response("ä½ å¥½"):
    print(response, flush = True, end = "")
```

å¦å¤–è¿˜å¯ä»¥è®¾ç½®cpuçº¿ç¨‹æ•°ç­‰å†…å®¹ï¼Œè¯¦ç»†APIè¯´æ˜è§ [fastllm_pytools](docs/fastllm_pytools.md)

è¿™ä¸ªåŒ…ä¸åŒ…å«low level apiï¼Œå¦‚æœéœ€è¦ä½¿ç”¨æ›´æ·±å…¥çš„åŠŸèƒ½è¯·å‚è€ƒ [Pythonç»‘å®šAPI](#Pythonç»‘å®šAPI)


## Pythonç»‘å®šAPI

```
cd pyfastllm
export USE_CUDA=OFF    # åªä½¿ç”¨CPUï¼Œå¦‚éœ€ä½¿ç”¨GPUåˆ™å»é™¤æœ¬è¡Œ
python3 setup.py build
python3 setup.py install 
cd examples/
python cli_simple.py  -m chatglm -p chatglm-6b-int8.flm æˆ–  
python web_api.py  -m chatglm -p chatglm-6b-int8.flm  
```
ä¸Šè¿°web apiå¯ä½¿ç”¨`web_api_client.py`è¿›è¡Œæµ‹è¯•ã€‚æ›´å¤šç”¨æ³•ï¼Œè¯¦è§[APIæ–‡æ¡£](pyfastllm/README.md)ã€‚

## å¤šå¡éƒ¨ç½²

### fastllm_pytoolsä¸­ä½¿ç”¨å¤šå¡éƒ¨ç½²

``` python

from fastllm_pytools import llm
# æ”¯æŒä¸‹åˆ—ä¸‰ç§æ–¹å¼ï¼Œéœ€è¦åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è°ƒç”¨
llm.set_device_map("cuda:0") # å°†æ¨¡å‹éƒ¨ç½²åœ¨å•ä¸€è®¾å¤‡ä¸Š
llm.set_device_map(["cuda:0", "cuda:1"]) # å°†æ¨¡å‹å¹³å‡éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
llm.set_device_map({"cuda:0" : 10, "cuda:1" : 5, "cpu": 1}) # å°†æ¨¡å‹æŒ‰ä¸åŒæ¯”ä¾‹éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š

```

### Pythonç»‘å®šAPIä¸­ä½¿ç”¨å¤šå¡éƒ¨ç½²

``` python
import pyfastllm as llm
# æ”¯æŒä»¥ä¸‹æ–¹å¼ï¼Œéœ€è¦åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è°ƒç”¨
llm.set_device_map({"cuda:0" : 10, "cuda:1" : 5, "cpu": 1}) # å°†æ¨¡å‹æŒ‰ä¸åŒæ¯”ä¾‹éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
```

### c++ä¸­ä½¿ç”¨å¤šå¡éƒ¨ç½²

``` cpp
// æ”¯æŒä»¥ä¸‹æ–¹å¼ï¼Œéœ€è¦åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è°ƒç”¨
fastllm::SetDeviceMap({{"cuda:0", 10}, {"cuda:1", 5}, {"cpu", 1}}); // å°†æ¨¡å‹æŒ‰ä¸åŒæ¯”ä¾‹éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
```

## Docker ç¼–è¯‘è¿è¡Œ
docker è¿è¡Œéœ€è¦æœ¬åœ°å®‰è£…å¥½ NVIDIA Runtime,ä¸”ä¿®æ”¹é»˜è®¤ runtime ä¸º nvidia

1. å®‰è£… nvidia-container-runtime
```
sudo apt-get install nvidia-container-runtime
```

2. ä¿®æ”¹ docker é»˜è®¤ runtime ä¸º nvidia

/etc/docker/daemon.json
```
{
  "registry-mirrors": [
    "https://hub-mirror.c.163.com",
    "https://mirror.baidubce.com"
  ],
  "runtimes": {
      "nvidia": {
          "path": "/usr/bin/nvidia-container-runtime",
          "runtimeArgs": []
      }
   },
   "default-runtime": "nvidia" // æœ‰è¿™ä¸€è¡Œå³å¯
}

```

3. ä¸‹è½½å·²ç»è½¬å¥½çš„æ¨¡å‹åˆ° models ç›®å½•ä¸‹
```
models
  chatglm2-6b-fp16.flm
  chatglm2-6b-int8.flm
```

4. ç¼–è¯‘å¹¶å¯åŠ¨ webui
```
DOCKER_BUILDKIT=0 docker compose up -d --build
```

## Androidä¸Šä½¿ç”¨

### ç¼–è¯‘
``` sh
# åœ¨PCä¸Šç¼–è¯‘éœ€è¦ä¸‹è½½NDKå·¥å…·
# è¿˜å¯ä»¥å°è¯•ä½¿ç”¨æ‰‹æœºç«¯ç¼–è¯‘ï¼Œåœ¨termuxä¸­å¯ä»¥ä½¿ç”¨cmakeå’Œgccï¼ˆä¸éœ€è¦ä½¿ç”¨NDKï¼‰
mkdir build-android
cd build-android
export NDK=<your_ndk_directory>
# å¦‚æœæ‰‹æœºä¸æ”¯æŒï¼Œé‚£ä¹ˆå»æ‰ "-DCMAKE_CXX_FLAGS=-march=armv8.2a+dotprod" ï¼ˆæ¯”è¾ƒæ–°çš„æ‰‹æœºéƒ½æ˜¯æ”¯æŒçš„ï¼‰
cmake -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-23 -DCMAKE_CXX_FLAGS=-march=armv8.2a+dotprod ..
make -j
```

### è¿è¡Œ

1. åœ¨Androidè®¾å¤‡ä¸Šå®‰è£…termuxè½¯ä»¶
2. åœ¨termuxä¸­æ‰§è¡Œtermux-setup-storageè·å¾—è¯»å–æ‰‹æœºæ–‡ä»¶çš„æƒé™ã€‚
3. å°†NDKç¼–è¯‘å‡ºçš„mainæ–‡ä»¶ï¼Œä»¥åŠæ¨¡å‹æ–‡ä»¶å­˜å…¥æ‰‹æœºï¼Œå¹¶æ‹·è´åˆ°termuxçš„æ ¹ç›®å½•
4. ä½¿ç”¨å‘½ä»¤```chmod 777 main```èµ‹æƒ
5. ç„¶åå¯ä»¥è¿è¡Œmainæ–‡ä»¶ï¼Œå‚æ•°æ ¼å¼å‚è§```./main --help```

## æ¨¡å‹è·å–

### æ¨¡å‹åº“

å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥ä¸­ä¸‹è½½å·²ç»è½¬æ¢å¥½çš„æ¨¡å‹

[huggingface](https://huggingface.co/huangyuyang) 

### æ¨¡å‹å¯¼å‡º

#### ChatGLMæ¨¡å‹å¯¼å‡º (é»˜è®¤è„šæœ¬å¯¼å‡ºChatGLM2-6bæ¨¡å‹)

``` sh
# éœ€è¦å…ˆå®‰è£…ChatGLM-6Bç¯å¢ƒ
# å¦‚æœä½¿ç”¨è‡ªå·±finetuneçš„æ¨¡å‹éœ€è¦ä¿®æ”¹chatglm_export.pyæ–‡ä»¶ä¸­åˆ›å»ºtokenizer, modelçš„ä»£ç 
cd build
python3 tools/chatglm_export.py chatglm2-6b-fp16.flm float16 #å¯¼å‡ºfloat16æ¨¡å‹
python3 tools/chatglm_export.py chatglm2-6b-int8.flm int8 #å¯¼å‡ºint8æ¨¡å‹
python3 tools/chatglm_export.py chatglm2-6b-int4.flm int4 #å¯¼å‡ºint4æ¨¡å‹
```

#### baichuanæ¨¡å‹å¯¼å‡º (é»˜è®¤è„šæœ¬å¯¼å‡ºbaichuan-13b-chatæ¨¡å‹)

``` sh
# éœ€è¦å…ˆå®‰è£…baichuanç¯å¢ƒ
# å¦‚æœä½¿ç”¨è‡ªå·±finetuneçš„æ¨¡å‹éœ€è¦ä¿®æ”¹baichuan2flm.pyæ–‡ä»¶ä¸­åˆ›å»ºtokenizer, modelçš„ä»£ç 
# æ ¹æ®æ‰€éœ€çš„ç²¾åº¦ï¼Œå¯¼å‡ºç›¸åº”çš„æ¨¡å‹
cd build
python3 tools/baichuan2flm.py baichuan-13b-fp16.flm float16 #å¯¼å‡ºfloat16æ¨¡å‹
python3 tools/baichuan2flm.py baichuan-13b-int8.flm int8 #å¯¼å‡ºint8æ¨¡å‹
python3 tools/baichuan2flm.py baichuan-13b-int4.flm int4 #å¯¼å‡ºint4æ¨¡å‹
```

#### baichuan2æ¨¡å‹å¯¼å‡º (é»˜è®¤è„šæœ¬å¯¼å‡ºbaichuan2-7b-chatæ¨¡å‹)

``` sh
# éœ€è¦å…ˆå®‰è£…baichuan2ç¯å¢ƒ
# å¦‚æœä½¿ç”¨è‡ªå·±finetuneçš„æ¨¡å‹éœ€è¦ä¿®æ”¹baichuan2_2flm.pyæ–‡ä»¶ä¸­åˆ›å»ºtokenizer, modelçš„ä»£ç 
# æ ¹æ®æ‰€éœ€çš„ç²¾åº¦ï¼Œå¯¼å‡ºç›¸åº”çš„æ¨¡å‹
cd build
python3 tools/baichuan2_2flm.py baichuan2-7b-fp16.flm float16 #å¯¼å‡ºfloat16æ¨¡å‹
python3 tools/baichuan2_2flm.py baichuan2-7b-int8.flm int8 #å¯¼å‡ºint8æ¨¡å‹
python3 tools/baichuan2_2flm.py baichuan2-7b-int4.flm int4 #å¯¼å‡ºint4æ¨¡å‹
```

#### MOSSæ¨¡å‹å¯¼å‡º

``` sh
# éœ€è¦å…ˆå®‰è£…MOSSç¯å¢ƒ
# å¦‚æœä½¿ç”¨è‡ªå·±finetuneçš„æ¨¡å‹éœ€è¦ä¿®æ”¹moss_export.pyæ–‡ä»¶ä¸­åˆ›å»ºtokenizer, modelçš„ä»£ç 
# æ ¹æ®æ‰€éœ€çš„ç²¾åº¦ï¼Œå¯¼å‡ºç›¸åº”çš„æ¨¡å‹
cd build
python3 tools/moss_export.py moss-fp16.flm float16 #å¯¼å‡ºfloat16æ¨¡å‹
python3 tools/moss_export.py moss-int8.flm int8 #å¯¼å‡ºint8æ¨¡å‹
python3 tools/moss_export.py moss-int4.flm int4 #å¯¼å‡ºint4æ¨¡å‹
```

#### LLAMAç³»åˆ—æ¨¡å‹å¯¼å‡º
``` sh
# ä¿®æ”¹build/tools/alpaca2flm.pyç¨‹åºè¿›è¡Œå¯¼å‡º
# ä¸åŒllamaæ¨¡å‹ä½¿ç”¨çš„æŒ‡ä»¤ç›¸å·®å¾ˆå¤§ï¼Œéœ€è¦å‚ç…§torch2flm.pyä¸­çš„å‚æ•°è¿›è¡Œé…ç½®
```
ä¸€äº›æ¨¡å‹çš„è½¬æ¢å¯ä»¥[å‚è€ƒè¿™é‡Œçš„ä¾‹å­](docs/llama_cookbook.md)

#### QWENæ¨¡å‹å¯¼å‡º
* **Qwen**
```sh
# éœ€è¦å…ˆå®‰è£…QWenç¯å¢ƒ
# å¦‚æœä½¿ç”¨è‡ªå·±finetuneçš„æ¨¡å‹éœ€è¦ä¿®æ”¹qwen2flm.pyæ–‡ä»¶ä¸­åˆ›å»ºtokenizer, modelçš„ä»£ç 
# æ ¹æ®æ‰€éœ€çš„ç²¾åº¦ï¼Œå¯¼å‡ºç›¸åº”çš„æ¨¡å‹
cd build
python3 tools/qwen2flm.py qwen-7b-fp16.flm float16 #å¯¼å‡ºfloat16æ¨¡å‹
python3 tools/qwen2flm.py qwen-7b-int8.flm int8 #å¯¼å‡ºint8æ¨¡å‹
python3 tools/qwen2flm.py qwen-7b-int4.flm int4 #å¯¼å‡ºint4æ¨¡å‹
```

* **Qwen1.5**

```sh
# éœ€è¦å…ˆå®‰è£…QWen2ç¯å¢ƒï¼ˆtransformers >= 4.37.0ï¼‰
# æ ¹æ®æ‰€éœ€çš„ç²¾åº¦ï¼Œå¯¼å‡ºç›¸åº”çš„æ¨¡å‹
cd build
python3 tools/llamalike2flm.py qwen1.5-7b-fp16.flm float16 "qwen/Qwen1.5-4B-Chat" #å¯¼å‡ºwen1.5-4B-Chat float16æ¨¡å‹
python3 tools/llamalike2flm.py qwen1.5-7b-int8.flm int8 "qwen/Qwen1.5-7B-Chat" #å¯¼å‡ºQwen1.5-7B-Chat int8æ¨¡å‹
python3 tools/llamalike2flm.py qwen1.5-7b-int4.flm int4 "qwen/Qwen1.5-14B-Chat" #å¯¼å‡ºQwen1.5-14B-Chat int4æ¨¡å‹
# æœ€åä¸€ä¸ªå‚æ•°å¯æ›¿æ¢ä¸ºæ¨¡å‹è·¯å¾„
```

#### MINICPMæ¨¡å‹å¯¼å‡º
```sh
# éœ€è¦å…ˆå®‰è£…MiniCPMç¯å¢ƒï¼ˆtransformers >= 4.36.0ï¼‰ 
# é»˜è®¤è„šæœ¬å¯¼å‡ºiniCPM-2B-dpo-fp16æ¨¡å‹
cd build 
python tools/minicpm2flm.py minicpm-2b-float16.flm #å¯¼å‡ºdpo-float16æ¨¡å‹
./main -p minicpm-2b-float16.flm # æ‰§è¡Œæ¨¡å‹
```

## å¼€å‘è®¡åˆ’

ä¹Ÿå°±æ˜¯ä¿—ç§°çš„ç”»é¥¼éƒ¨åˆ†ï¼Œå¤§å®¶å¦‚æœæœ‰éœ€è¦çš„åŠŸèƒ½å¯ä»¥åœ¨è®¨è®ºåŒºæå‡º

### çŸ­æœŸè®¡åˆ’

- æ·»åŠ MMLU, CMMLUç­‰æµ‹è¯•ç¨‹åº
- æ”¯æŒç›´æ¥è½¬æ¢å·²ç»é‡åŒ–å¥½çš„huggingfaceæ¨¡å‹
- å®ç°å¤–æ¨åˆ°8Ké•¿åº¦

### ä¸­æœŸè®¡åˆ’

- æ”¯æŒæ›´å¤šåç«¯ï¼Œå¦‚opencl, vulkan, ä»¥åŠä¸€äº›NPUåŠ é€Ÿè®¾å¤‡
- æ”¯æŒã€éªŒè¯æ›´å¤šæ¨¡å‹ï¼Œå®Œå–„æ¨¡å‹åº“
- ä¼˜åŒ–tokenizer (ç”±äºç›®å‰åœ¨pythonä¸­å¯ä»¥ç›´æ¥ä½¿ç”¨åŸæ¨¡å‹çš„tokenizeræ¥åˆ†è¯ï¼Œæ‰€ä»¥è¿™é¡¹å·¥ä½œæš‚æ—¶å¹¶ä¸æ€¥è¿«)

### é•¿æœŸè®¡åˆ’

- æ”¯æŒONNXæ¨¡å‹å¯¼å…¥ã€æ¨ç†
- æ”¯æŒæ¨¡å‹å¾®è°ƒ
