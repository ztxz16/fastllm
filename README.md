# fastllm

[English Document](README_EN.md)

## ä»‹ç»

fastllmæ˜¯çº¯c++å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ–çš„å¤šå¹³å°é«˜æ€§èƒ½å¤§æ¨¡å‹æ¨ç†åº“

éƒ¨ç½²äº¤æµQQç¾¤ï¼š 831641348

| [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) | [æ¨¡å‹è·å–](#æ¨¡å‹è·å–) |

## åŠŸèƒ½æ¦‚è¿°

- ğŸš€ çº¯c++å®ç°ï¼Œä¾¿äºè·¨å¹³å°ç§»æ¤ï¼Œå¯ä»¥åœ¨å®‰å“ä¸Šç›´æ¥ç¼–è¯‘
- ğŸš€ æ”¯æŒè¯»å–Hugging faceåŸå§‹æ¨¡å‹å¹¶ç›´æ¥é‡åŒ–
- ğŸš€ æ”¯æŒéƒ¨ç½²Openai api server
- ğŸš€ æ”¯æŒå¤šå¡éƒ¨ç½²ï¼Œæ”¯æŒGPU + CPUæ··åˆéƒ¨ç½²
- ğŸš€ æ”¯æŒåŠ¨æ€Batchï¼Œæµå¼è¾“å‡º
- ğŸš€ å‰åç«¯åˆ†ç¦»è®¾è®¡ï¼Œä¾¿äºæ”¯æŒæ–°çš„è®¡ç®—è®¾å¤‡
- ğŸš€ ç›®å‰æ”¯æŒChatGLMç³»åˆ—æ¨¡å‹ï¼ŒQwen2ç³»åˆ—æ¨¡å‹ï¼Œå„ç§LLAMAæ¨¡å‹(ALPACA, VICUNAç­‰)ï¼ŒBAICHUANæ¨¡å‹ï¼ŒMOSSæ¨¡å‹ï¼ŒMINICPMæ¨¡å‹ç­‰

## å¿«é€Ÿå¼€å§‹

### ç¼–è¯‘

å»ºè®®ä½¿ç”¨cmakeç¼–è¯‘ï¼Œéœ€è¦æå‰å®‰è£…gccï¼Œg++ (å»ºè®®9.4ä»¥ä¸Š), make, cmake (å»ºè®®3.23ä»¥ä¸Š)

GPUç¼–è¯‘éœ€è¦æå‰å®‰è£…å¥½CUDAç¼–è¯‘ç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨å°½å¯èƒ½æ–°çš„CUDAç‰ˆæœ¬

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ç¼–è¯‘

``` sh
bash install.sh -DUSE_CUDA=ON # ç¼–è¯‘GPUç‰ˆæœ¬
# bash install.sh -DUSE_CUDA=ON -DCUDA_ARCH=89 # å¯ä»¥æŒ‡å®šCUDAæ¶æ„ï¼Œå¦‚4090ä½¿ç”¨89æ¶æ„
# bash install.sh # ä»…ç¼–è¯‘CPUç‰ˆæœ¬
```

å…¶ä»–ä¸åŒå¹³å°çš„ç¼–è¯‘å¯å‚è€ƒæ–‡æ¡£
[TFACCå¹³å°](docs/tfacc.md)

### è¿è¡Œdemoç¨‹åº (python)

å‡è®¾æˆ‘ä»¬çš„æ¨¡å‹ä½äº"~/Qwen2-7B-Instruct/"ç›®å½•

ç¼–è¯‘å®Œæˆåå¯ä»¥ä½¿ç”¨ä¸‹åˆ—demo:

``` sh
# ä½¿ç”¨float16ç²¾åº¦çš„æ¨¡å‹å¯¹è¯
python3 -m ftllm.chat -t 16 -p ~/Qwen2-7B-Instruct/ 

# åœ¨çº¿é‡åŒ–ä¸ºint8æ¨¡å‹å¯¹è¯
python3 -m ftllm.chat -t 16 -p ~/Qwen2-7B-Instruct/ --dtype int8

# openai api server (ç›®å‰å¤„äºæµ‹è¯•è°ƒä¼˜é˜¶æ®µ)
# éœ€è¦å®‰è£…ä¾èµ–: pip install -r requirements-server.txt
# è¿™é‡Œåœ¨8080ç«¯å£æ‰“å¼€äº†ä¸€ä¸ªæ¨¡å‹åä¸ºqwençš„server
python3 -m ftllm.server -t 16 -p ~/Qwen2-7B-Instruct/ --port 8080 --model_name qwen

# webui
# éœ€è¦å®‰è£…ä¾èµ–: pip install streamlit-chat
python3 -m ftllm.webui -t 16 -p ~/Qwen2-7B-Instruct/ --port 8080
```

ä»¥ä¸Šdemoå‡å¯ä½¿ç”¨å‚æ•° --help æŸ¥çœ‹è¯¦ç»†å‚æ•°

ç›®å‰æ¨¡å‹çš„æ”¯æŒæƒ…å†µè§: [æ¨¡å‹åˆ—è¡¨](docs/models.md)

æœ‰ä¸€äº›æ¶æ„æš‚æ—¶æ— æ³•ç›´æ¥è¯»å–Hugging faceæ¨¡å‹ï¼Œå¯ä»¥å‚è€ƒ [æ¨¡å‹è½¬æ¢æ–‡æ¡£](docs/convert_model.md) è½¬æ¢fastllmæ ¼å¼çš„æ¨¡å‹

### è¿è¡Œdemoç¨‹åº (c++)

```
# è¿›å…¥fastllm/build-fastllmç›®å½•

# å‘½ä»¤è¡ŒèŠå¤©ç¨‹åº, æ”¯æŒæ‰“å­—æœºæ•ˆæœ (åªæ”¯æŒLinuxï¼‰
./main -p ~/Qwen2-7B-Instruct/ 

# ç®€æ˜“webui, ä½¿ç”¨æµå¼è¾“å‡º + åŠ¨æ€batchï¼Œå¯å¤šè·¯å¹¶å‘è®¿é—®
./webui -p ~/Qwen2-7B-Instruct/ --port 1234 
```

Windowsä¸‹çš„ç¼–è¯‘æ¨èä½¿ç”¨Cmake GUI + Visual Studioï¼Œåœ¨å›¾å½¢åŒ–ç•Œé¢ä¸­å®Œæˆã€‚

å¦‚ç¼–è¯‘ä¸­å­˜åœ¨é—®é¢˜ï¼Œå°¤å…¶æ˜¯Windowsä¸‹çš„ç¼–è¯‘ï¼Œå¯å‚è€ƒ[FAQ](docs/faq.md)

### python API

``` python
# æ¨¡å‹åˆ›å»º
from ftllm import llm
model = llm.model("~/Qwen2-7B-Instruct/")

# ç”Ÿæˆå›å¤
print(model.response("ä½ å¥½"))

# æµå¼ç”Ÿæˆå›å¤
for response in model.stream_response("ä½ å¥½"):
    print(response, flush = True, end = "")
```

å¦å¤–è¿˜å¯ä»¥è®¾ç½®cpuçº¿ç¨‹æ•°ç­‰å†…å®¹ï¼Œè¯¦ç»†APIè¯´æ˜è§ [ftllm](docs/ftllm.md)

è¿™ä¸ªåŒ…ä¸åŒ…å«low level apiï¼Œå¦‚æœéœ€è¦ä½¿ç”¨æ›´æ·±å…¥çš„åŠŸèƒ½è¯·å‚è€ƒ [Pythonç»‘å®šAPI](#Pythonç»‘å®šAPI)

## å¤šå¡éƒ¨ç½²

### pythonå‘½ä»¤è¡Œè°ƒç”¨ä¸­ä½¿ç”¨å¤šå¡éƒ¨ç½²

``` sh
# ä½¿ç”¨å‚æ•°--deviceæ¥è®¾ç½®å¤šå¡è°ƒç”¨
#--device cuda:1 # è®¾ç½®å•ä¸€è®¾å¤‡
#--device "['cuda:0', 'cuda:1']" # å°†æ¨¡å‹å¹³å‡éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
#--device "{'cuda:0': 10, 'cuda:1': 5, 'cpu': 1} # å°†æ¨¡å‹æŒ‰ä¸åŒæ¯”ä¾‹éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
```
### ftllmä¸­ä½¿ç”¨å¤šå¡éƒ¨ç½²

``` python
from ftllm import llm
# æ”¯æŒä¸‹åˆ—ä¸‰ç§æ–¹å¼ï¼Œéœ€è¦åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è°ƒç”¨
llm.set_device_map("cuda:0") # å°†æ¨¡å‹éƒ¨ç½²åœ¨å•ä¸€è®¾å¤‡ä¸Š
llm.set_device_map(["cuda:0", "cuda:1"]) # å°†æ¨¡å‹å¹³å‡éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
llm.set_device_map({"cuda:0" : 10, "cuda:1" : 5, "cpu": 1}) # å°†æ¨¡å‹æŒ‰ä¸åŒæ¯”ä¾‹éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
```

### Pythonç»‘å®šAPIä¸­ä½¿ç”¨å¤šå¡éƒ¨ç½²

``` python
import pyfastllm as llm
# æ”¯æŒä»¥ä¸‹æ–¹å¼ï¼Œéœ€è¦åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è°ƒç”¨
llm.set_device_map({"cuda:0" : 10, "cuda:1" : 5, "cpu": 1}) # å°†æ¨¡å‹æŒ‰ä¸åŒæ¯”ä¾‹éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
```

### c++ä¸­ä½¿ç”¨å¤šå¡éƒ¨ç½²

``` cpp
// æ”¯æŒä»¥ä¸‹æ–¹å¼ï¼Œéœ€è¦åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è°ƒç”¨
fastllm::SetDeviceMap({{"cuda:0", 10}, {"cuda:1", 5}, {"cpu", 1}}); // å°†æ¨¡å‹æŒ‰ä¸åŒæ¯”ä¾‹éƒ¨ç½²åœ¨å¤šä¸ªè®¾å¤‡ä¸Š
```

## Docker ç¼–è¯‘è¿è¡Œ
docker è¿è¡Œéœ€è¦æœ¬åœ°å®‰è£…å¥½ NVIDIA Runtime,ä¸”ä¿®æ”¹é»˜è®¤ runtime ä¸º nvidia

1. å®‰è£… nvidia-container-runtime
```
sudo apt-get install nvidia-container-runtime
```

2. ä¿®æ”¹ docker é»˜è®¤ runtime ä¸º nvidia

/etc/docker/daemon.json
```
{
  "registry-mirrors": [
    "https://hub-mirror.c.163.com",
    "https://mirror.baidubce.com"
  ],
  "runtimes": {
      "nvidia": {
          "path": "/usr/bin/nvidia-container-runtime",
          "runtimeArgs": []
      }
   },
   "default-runtime": "nvidia" // æœ‰è¿™ä¸€è¡Œå³å¯
}

```

3. ä¸‹è½½å·²ç»è½¬å¥½çš„æ¨¡å‹åˆ° models ç›®å½•ä¸‹
```
models
  chatglm2-6b-fp16.flm
  chatglm2-6b-int8.flm
```

4. ç¼–è¯‘å¹¶å¯åŠ¨ webui
```
DOCKER_BUILDKIT=0 docker compose up -d --build
```

## Androidä¸Šä½¿ç”¨

### ç¼–è¯‘
``` sh
# åœ¨PCä¸Šç¼–è¯‘éœ€è¦ä¸‹è½½NDKå·¥å…·
# è¿˜å¯ä»¥å°è¯•ä½¿ç”¨æ‰‹æœºç«¯ç¼–è¯‘ï¼Œåœ¨termuxä¸­å¯ä»¥ä½¿ç”¨cmakeå’Œgccï¼ˆä¸éœ€è¦ä½¿ç”¨NDKï¼‰
mkdir build-android
cd build-android
export NDK=<your_ndk_directory>
# å¦‚æœæ‰‹æœºä¸æ”¯æŒï¼Œé‚£ä¹ˆå»æ‰ "-DCMAKE_CXX_FLAGS=-march=armv8.2a+dotprod" ï¼ˆæ¯”è¾ƒæ–°çš„æ‰‹æœºéƒ½æ˜¯æ”¯æŒçš„ï¼‰
cmake -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-23 -DCMAKE_CXX_FLAGS=-march=armv8.2a+dotprod ..
make -j
```

### è¿è¡Œ

1. åœ¨Androidè®¾å¤‡ä¸Šå®‰è£…termuxè½¯ä»¶
2. åœ¨termuxä¸­æ‰§è¡Œtermux-setup-storageè·å¾—è¯»å–æ‰‹æœºæ–‡ä»¶çš„æƒé™ã€‚
3. å°†NDKç¼–è¯‘å‡ºçš„mainæ–‡ä»¶ï¼Œä»¥åŠæ¨¡å‹æ–‡ä»¶å­˜å…¥æ‰‹æœºï¼Œå¹¶æ‹·è´åˆ°termuxçš„æ ¹ç›®å½•
4. ä½¿ç”¨å‘½ä»¤```chmod 777 main```èµ‹æƒ
5. ç„¶åå¯ä»¥è¿è¡Œmainæ–‡ä»¶ï¼Œå‚æ•°æ ¼å¼å‚è§```./main --help```

